
如果说某个topic一开始数据量很小，比如每天就几百MB的数据，那你不需要给他分配过多的分区，因为你要是搞10个分区，分散在10台
机器上，每台机器就几十MB的数据，有什么意义呢?

所以一般建议生产环境采取的策略是刚开始就是按照预估的数据量给合理的分区数即可，通常来说，你可以限制每个分区在几个GB到
10个GB，或者最多20个GB左右，都可以，具体要看你们公司的Kafka集群的机器资源情况

那么如果后来业务不停的发展，发现topic数据量太大了，当前的几个分区所在机器有点压力了，需要扩容增加更多的topic呢？这个
时候你就可以动态扩容分区了，这样就可以让新的数据慢慢分散到更多的分区，更多的机器上去

bin/kafka-topics.sh --**alter** --zookeeper localhost:2181 --partitions 10 --topic test-topic
