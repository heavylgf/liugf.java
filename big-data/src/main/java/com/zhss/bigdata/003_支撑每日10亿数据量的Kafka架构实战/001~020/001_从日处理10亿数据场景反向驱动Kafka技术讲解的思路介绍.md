
Kafka技术，讲解的思路

按部就班的思路去讲解的话，纯讲干讲kafka自己技术本身，一块一块的讲，效果并不是太好，hdfs，在大数据平台里面主要是搭建和
部署好了之后，其实平时很少是我们自己直接去使用他的

hdfs那块就核心和关键的一点，他其实是异常、性能，所以需要深入研究里面的源码，然后对源码有二次开发的能力，去解决相应的问题

平时我们很有可能是会自己直接用到kafka的，实时计算，storm，spark streaming，flink，是一个数据流处理的管道，这里无论你
是用什么技术来做实时计算，你先从kafka里消费出来数据，处理，接着再把数据写回到kafka里面

hdfs，除非你是改动spark、hive、hbase底层的源码，才有可能会牵扯到你直接自己基于代码来操作hdfs，一般来说都是一些在hdfs上
层的大数据系统，他们会去使用到hdfs，不是直接用hdfs，解决他的一些故障和问题

不能上来就直接kafka源码，然后讲一些源码的二次开发，跟平时日常工作中大家对kafka的使用场景就不匹配了

咱们先构造一个场景出来，模拟出来一个场景，也是基于我真实的互联网公司的从业经验，把数据量减少，简化一下，抽象出来一个模拟的
场景，1000万用户的电商平台，每天会产生各种数据会有多少

每天各种数据加起来有10亿，涌入kafka里面，作为一个实时数据流，供下游的一些实时分析系统来抽取数据流分析

每秒钟会涌入多少数据，需要支撑多大的吞吐量，包括每天有多少数据需要落地在磁盘来存储，集群需要存储多大的数据量

kafka架构原理来入手，至少两周的时间，你就知道kafka整体是怎么回事了
