
小公司，20个人的数据团队，其中就几个人写的实时的应用会来使用kafka，那么其实还是真的还好，你平时作为一个kafka的管理员，
多看看topic的数据增长，很多团队都接入了kafka，在里面创建了成百上千个topic

你的每一个topic的初始分区的数量是通过人家填写的表单来估算的，每天有多少GB的数据，你就可以预估一下大概需要几个分区，
10GB以内的数据，可以刚开始就给他一个分区也可以了

其实通过kafka管理员手动执行命令扩容topic分区数量，是可以的，但是仅限于中小公司，因为业务方不多，没那么复杂，但是如果在
中大型公司，那么就是不可能的了，此时就需要有一个平台可以全自动的去监控各个topic的分区数据量的增长，以及自动进行分区数量
的扩容

其实也很简单的，你在你的kafka运维管理平台里写一个后台自己每天凌晨运行的程序，他自己每天去扫描一下每个topic的数据量，
跟最开始的那个数据量进行比对，比如一个topic刚开始上线一天是10GB，但是后来慢慢发现变成一天有20GB了

那么此时，你就可以程序自动扩容，自动调用kafka api来修改那个topic的分区数，提升一倍即可，这个api给大家留个作业自己去找，
自己动手写一下代码，其实非常非常的简单，自己百度百度，查查官方文档

一开始某个topic每天就3GB的数据，一开始给了一个分区

后来呢你每天都通过API扫描这个topic的数据量，后来慢慢的发现这个topic的数据量怎么变成了6GB，业务在增长，自动后台调用一个
API提高这个topic的分区数量，变成2了，利用两台机器的资源来存放这个topic了

稍微花点时间，封装一个好用的kafka运维管理平台，人家自助式接入创建topic，还可以自动监控topic数据量的增长，自动进行topic
的分区扩容，对业务的支撑，全部都实现web平台的自助式，自动化


