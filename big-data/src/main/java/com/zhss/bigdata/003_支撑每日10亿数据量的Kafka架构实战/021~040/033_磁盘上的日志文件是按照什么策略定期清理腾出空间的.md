

大家可以想，不可能说每天涌入的数据都一直留存在磁盘上，本质kafka是一个流式数据的中间件，不需要跟离线存储系统一样保存全量的大数据，
所以kafka是会定期清理掉数据的，这里有几个清理策略

kafka默认是保留最近7天的数据，每天都会把7天以前的数据给清理掉，包括.log、.index和.timeindex几个文件，log.retention.hours
参数，可以自己设置数据要保留多少天，你可以根据自己线上的场景来判断一下

只要你的数据保留在kafka里，你随时可以通过offset的指定，随时可以从kafka楼出来几天之前的数据，数据回放一遍，下游的数据，
有多么的重要，如果是特别核心的数据，在kafka这个层面，可以保留7天，甚至是15天的数据

下游的消费者消费了数据之后，数据丢失了，你需要从kafka里楼出来3天前的数据，重新来回放处理一遍

在大数据的实时分析的项目里，其实就会涉及到这个东西的一个使用，如果你今天实时分析的一些数据出错了，此时你就需要把过去几天的数据
重新楼出来回放一遍，重新来算一遍。实时数据分析的结果和hadoop离线分析的结果做一个比对

你每天都会从kafka里搂出来几天前的数据，算一下，跟离线数据的结果做一个比对

kafka broker会在后台启动线程异步的进行日志清理的工作

异步的进行日志的清理


