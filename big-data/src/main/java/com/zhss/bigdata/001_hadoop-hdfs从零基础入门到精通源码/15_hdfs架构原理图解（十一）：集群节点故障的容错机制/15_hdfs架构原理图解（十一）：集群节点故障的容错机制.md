
hdfs这样的一个架构下，集群如何保障某个节点故障时候的容错性呢？

（1）集群节点故障

比如说第一种故障情况：网络分区。啥叫做网络分区？英文就是network partition，说白了就是集群里网络故障了，一部分datanode跟namenode无法痛点了，此时网络环境不就相当于是分成了两块儿了，这就是所谓的网络分区。s

因为master-slave架构的分布式系统，一般都会设计心跳机制，就是datanode会定时发送心跳以及block report到namenode去，那如果网络分区了，namenode肯定会感知到的，因为一部分datanode心跳没发送过来了。

这个时候namenode就会将这些无法发送心跳的datanode标记为dead状态，已经死掉了，然后就不会再让hdfs客户端去读写那些datanode了。默认是10min接收不到心跳才会标记datanode死掉了。而且这个时候datanode上的一些block不就不可用了么？这个时候namenode会检测到，然后会发现一些block的replica副本就不够了，那么此时namenode就会让其他的datanode去复制一些replica保证3副本。

除了这种网络分区以外，还有别的一些故障，比如说datanode所在机器宕机了，或者datanode进程就挂了，或者是那个block对应的文件损坏了，都会让namenode感知到，此时namenode会自动在集群里复制block，保证每个block的三副本。

（2）数据破损

此外还有一种机制，就是hdfs的数据完整性校验机制，在一个客户端上传一个文件到hdfs的时候，其实是会基于文件内容算一个校验和出来的，就是checksum，放到一个隐藏文件里去，也是在hdfs里的。

然后在读取文件内容的时候，会对读取到的文件内容重新算一个校验和，与之前上传时的校验和比对一下，如果不一样说明文件破损了，此时他会尝试对某个block读取其他的副本。

（3）元数据文件损坏

fsimage和edits log都是非常关键的元数据，如果这些文件损坏了，那么hdfs可能就无法正常工作了，如果要保护这个namenode的可用性，可以使用namenode HA部署双机进行热备，出现故障自动切换namenode。

为我们后续深入的研究hdfs的源码打下了坚实的基础，如果你脑子里没有这样的一张图的话，你后面根本没法看懂源码的

我在这个基础的原理图里面，有一些阐述和论断，不一定是完全的精准的，


