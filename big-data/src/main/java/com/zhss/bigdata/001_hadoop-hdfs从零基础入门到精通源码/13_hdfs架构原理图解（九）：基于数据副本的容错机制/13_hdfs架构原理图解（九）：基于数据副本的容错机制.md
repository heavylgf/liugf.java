
![](04_hdfs整体架构原理(8).png)

所以聊到这里，就有一个问题了，你想啊，比如说一个文件拆分成了10个block，分布在10台机器上，每个机器存储1个block，看起来还挺美的，但是问题在于说，如果某台机器故障了呢？那台机器啥故障都可能有，比如说啊，常见的，磁盘故障，网络故障，内存耗尽，CPU耗尽，或者干脆机器自己宕机了，防不上防啊

所以呢，如果说一个文件少了一个block，就9个block了，那数据不完整了，坑爹，这样人家读取文件都没法弄了，咋整呢？所以为了对集群中的各种机器的故障进行容错，有一个关键的机制，就是副本机制

所以说呢，你可以设置一个关键性的参数，叫做replication factor，号称是叫做复制因子，其实谁知道什么鬼因子的，说白了就是你的每个block要复制几份副本到其他的机器上去，那么如果某台机器挂了，这样好了啊，其他机器上有一模一样的block副本。这个replication factor可以整体设置一下，也可以对每个文件设置一下，然后后续还可以修改

那么在写文件的时候，假如说默认的每个block就是3副本，此时namenode会先根据一个复制算法挑选出来3个datanode，每个datanode放一个block，返回给客户端了。客户端先第一个datanode写入一个block，接着datanode将这个block复制给第二个datanode，然后第二个datanode再将block复制给第三个datanode。

rack aware

namenode在分配block到datanode时候，有一个关键性的机制，叫做机架感知特性，这啥意思呢？就是hdfs集群里的机器与机器之间不是肯定是要进行通信的么，然后一个机架上的机器之间通信速度，要比不同机架上的机器之间的通信速度快的多了。然后namenode比如默认一般一个block是3个副本，你就可以把2个副本放在一个机架上，然后第3个副本放在另外一个机架上。

这样的话，在一个机架里，同步复制2个副本，通信速度很好，仅仅只有一个副本是同步到另外一个机架上去的，确实会影响一点写的速度，但是总比你在三个机架上各方一个副本好吧；然后如果一个机架完全挂了，还有另外一个机架上有一个副本。

这个namenode呢，每隔一段时间就会从各个datanode那里获取一个heartbeat，这是用来确定那个datanode还活着的，所以叫做心跳；还有就是获取一份block report，就是每个datanode报告自己本地可用的block有哪些

这样的话，namenode就可以不断的知道整个集群中的block的情况啦，然后在新创建文件分配block给datanode的时候，不就可以根据各个datanode当前的block数量来均匀的分配了么

在读数据的时候，会优先找离自己最近的那个副本所在的机器，保证读取性能最高






